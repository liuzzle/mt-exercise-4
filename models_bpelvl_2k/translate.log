2025-05-28 16:26:45,824 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-28 16:26:45,843 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-28 16:26:45,884 - INFO - joeynmt.model - Enc-dec model built.
2025-05-28 16:26:45,916 - INFO - joeynmt.helpers - Load model from /Users/blueberry/Library/CloudStorage/OneDrive-UniversitätZürichUZH/Studium/FS25/MT/MT_exercises/MT_ex4/mt-exercise-4/models_bpelvl_2k/24000.ckpt.
2025-05-28 16:26:45,951 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 16:26:45,952 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 16:26:45,954 - INFO - joeynmt.prediction - Predicting 1567 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 16:31:52,899 - INFO - joeynmt.prediction - Generation took 306.9374[sec]. (No references given)
2025-05-28 16:33:09,971 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-28 16:33:09,991 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-28 16:33:10,031 - INFO - joeynmt.model - Enc-dec model built.
2025-05-28 16:33:10,065 - INFO - joeynmt.helpers - Load model from /Users/blueberry/Library/CloudStorage/OneDrive-UniversitätZürichUZH/Studium/FS25/MT/MT_exercises/MT_ex4/mt-exercise-4/models_bpelvl_2k/24000.ckpt.
2025-05-28 16:33:10,102 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 16:33:10,102 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 16:33:10,104 - INFO - joeynmt.prediction - Predicting 1567 example(s)... (Beam search with beam_size=3, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 16:36:13,595 - INFO - joeynmt.prediction - Generation took 183.4824[sec]. (No references given)
2025-05-28 16:36:58,798 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-28 16:36:58,818 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-28 16:36:58,856 - INFO - joeynmt.model - Enc-dec model built.
2025-05-28 16:36:58,888 - INFO - joeynmt.helpers - Load model from /Users/blueberry/Library/CloudStorage/OneDrive-UniversitätZürichUZH/Studium/FS25/MT/MT_exercises/MT_ex4/mt-exercise-4/models_bpelvl_2k/24000.ckpt.
2025-05-28 16:36:58,926 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 16:36:58,926 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 16:36:58,929 - INFO - joeynmt.prediction - Predicting 1567 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 16:38:03,338 - INFO - joeynmt.prediction - Generation took 64.3886[sec]. (No references given)
2025-05-28 16:38:37,023 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-28 16:38:37,044 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-28 16:38:37,088 - INFO - joeynmt.model - Enc-dec model built.
2025-05-28 16:38:37,124 - INFO - joeynmt.helpers - Load model from /Users/blueberry/Library/CloudStorage/OneDrive-UniversitätZürichUZH/Studium/FS25/MT/MT_exercises/MT_ex4/mt-exercise-4/models_bpelvl_2k/24000.ckpt.
2025-05-28 16:38:37,163 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 16:38:37,163 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 16:38:37,166 - INFO - joeynmt.prediction - Predicting 1567 example(s)... (Beam search with beam_size=2, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 16:40:34,979 - INFO - joeynmt.prediction - Generation took 117.8029[sec]. (No references given)
2025-05-28 16:41:11,449 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-28 16:41:11,470 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-28 16:41:11,506 - INFO - joeynmt.model - Enc-dec model built.
2025-05-28 16:41:11,540 - INFO - joeynmt.helpers - Load model from /Users/blueberry/Library/CloudStorage/OneDrive-UniversitätZürichUZH/Studium/FS25/MT/MT_exercises/MT_ex4/mt-exercise-4/models_bpelvl_2k/24000.ckpt.
2025-05-28 16:41:11,578 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 16:41:11,578 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 16:41:11,580 - INFO - joeynmt.prediction - Predicting 1567 example(s)... (Beam search with beam_size=4, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 16:44:53,913 - INFO - joeynmt.prediction - Generation took 222.3250[sec]. (No references given)
2025-05-28 16:45:22,201 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-28 16:45:22,221 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-28 16:45:22,264 - INFO - joeynmt.model - Enc-dec model built.
2025-05-28 16:45:22,297 - INFO - joeynmt.helpers - Load model from /Users/blueberry/Library/CloudStorage/OneDrive-UniversitätZürichUZH/Studium/FS25/MT/MT_exercises/MT_ex4/mt-exercise-4/models_bpelvl_2k/24000.ckpt.
2025-05-28 16:45:22,335 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 16:45:22,336 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 16:45:22,338 - INFO - joeynmt.prediction - Predicting 1567 example(s)... (Beam search with beam_size=6, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 16:51:16,045 - INFO - joeynmt.prediction - Generation took 353.7008[sec]. (No references given)
2025-05-28 16:51:52,398 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-28 16:51:52,418 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-28 16:51:52,441 - INFO - joeynmt.model - Enc-dec model built.
2025-05-28 16:51:52,472 - INFO - joeynmt.helpers - Load model from /Users/blueberry/Library/CloudStorage/OneDrive-UniversitätZürichUZH/Studium/FS25/MT/MT_exercises/MT_ex4/mt-exercise-4/models_bpelvl_2k/24000.ckpt.
2025-05-28 16:51:52,510 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 16:51:52,510 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 16:51:52,512 - INFO - joeynmt.prediction - Predicting 1567 example(s)... (Beam search with beam_size=7, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 16:58:41,643 - INFO - joeynmt.prediction - Generation took 409.1247[sec]. (No references given)
2025-05-28 17:00:09,041 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-28 17:00:09,060 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-28 17:00:09,084 - INFO - joeynmt.model - Enc-dec model built.
2025-05-28 17:00:09,114 - INFO - joeynmt.helpers - Load model from /Users/blueberry/Library/CloudStorage/OneDrive-UniversitätZürichUZH/Studium/FS25/MT/MT_exercises/MT_ex4/mt-exercise-4/models_bpelvl_2k/24000.ckpt.
2025-05-28 17:00:09,149 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 17:00:09,149 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 17:00:09,151 - INFO - joeynmt.prediction - Predicting 1567 example(s)... (Beam search with beam_size=8, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:07:51,660 - INFO - joeynmt.prediction - Generation took 462.5027[sec]. (No references given)
2025-05-28 17:08:25,055 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-28 17:08:25,075 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-28 17:08:25,101 - INFO - joeynmt.model - Enc-dec model built.
2025-05-28 17:08:25,131 - INFO - joeynmt.helpers - Load model from /Users/blueberry/Library/CloudStorage/OneDrive-UniversitätZürichUZH/Studium/FS25/MT/MT_exercises/MT_ex4/mt-exercise-4/models_bpelvl_2k/24000.ckpt.
2025-05-28 17:08:25,167 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 17:08:25,167 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 17:08:25,169 - INFO - joeynmt.prediction - Predicting 1567 example(s)... (Beam search with beam_size=9, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:17:04,025 - INFO - joeynmt.prediction - Generation took 518.8499[sec]. (No references given)
2025-05-28 17:17:30,332 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-28 17:17:30,352 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-28 17:17:30,376 - INFO - joeynmt.model - Enc-dec model built.
2025-05-28 17:17:30,406 - INFO - joeynmt.helpers - Load model from /Users/blueberry/Library/CloudStorage/OneDrive-UniversitätZürichUZH/Studium/FS25/MT/MT_exercises/MT_ex4/mt-exercise-4/models_bpelvl_2k/24000.ckpt.
2025-05-28 17:17:30,441 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 17:17:30,442 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 17:17:30,444 - INFO - joeynmt.prediction - Predicting 1567 example(s)... (Beam search with beam_size=10, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:27:09,017 - INFO - joeynmt.prediction - Generation took 578.5665[sec]. (No references given)
2025-05-28 17:39:30,600 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-28 17:39:30,620 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-28 17:39:30,660 - INFO - joeynmt.model - Enc-dec model built.
2025-05-28 17:39:30,694 - INFO - joeynmt.helpers - Load model from /Users/blueberry/Library/CloudStorage/OneDrive-UniversitätZürichUZH/Studium/FS25/MT/MT_exercises/MT_ex4/mt-exercise-4/models_bpelvl_2k/24000.ckpt.
2025-05-28 17:39:30,732 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 17:39:30,732 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 17:39:30,735 - INFO - joeynmt.prediction - Predicting 1567 example(s)... (Beam search with beam_size=11, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 17:50:07,169 - INFO - joeynmt.prediction - Generation took 636.4286[sec]. (No references given)
