2025-05-16 09:54:33,770 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-16 09:54:33,770 - INFO - joeynmt.helpers -                           cfg.name : transformer_sample_config
2025-05-16 09:54:33,770 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2025-05-16 09:54:33,770 - INFO - joeynmt.helpers -                     cfg.data.train : sampled_data/train.de-it
2025-05-16 09:54:33,770 - INFO - joeynmt.helpers -                       cfg.data.dev : sampled_data/dev.de-it
2025-05-16 09:54:33,770 - INFO - joeynmt.helpers -                      cfg.data.test : sampled_data/test.de-it
2025-05-16 09:54:33,770 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2025-05-16 09:54:33,770 - INFO - joeynmt.helpers -                  cfg.data.src.lang : de
2025-05-16 09:54:33,770 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2025-05-16 09:54:33,770 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2025-05-16 09:54:33,770 - INFO - joeynmt.helpers -       cfg.data.src.max_sent_length : 100
2025-05-16 09:54:33,770 - INFO - joeynmt.helpers -             cfg.data.src.voc_limit : 32000
2025-05-16 09:54:33,770 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : bpe_codes
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : it
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -       cfg.data.trg.max_sent_length : 100
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -             cfg.data.trg.voc_limit : 32000
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : bpe_codes
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -             cfg.training.model_dir : modelss
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -              cfg.training.use_cuda : False
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : False
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2025-05-16 09:54:33,771 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2025-05-16 09:54:33,772 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2025-05-16 09:54:33,773 - INFO - joeynmt.data - Building tokenizer...
2025-05-16 09:54:33,805 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-16 09:54:33,805 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-16 09:54:33,805 - INFO - joeynmt.data - Loading train set...
2025-05-16 09:54:33,885 - INFO - joeynmt.data - Building vocabulary...
2025-05-16 09:54:41,830 - INFO - joeynmt.data - Loading dev set...
2025-05-16 09:54:41,832 - INFO - joeynmt.data - Loading test set...
2025-05-16 09:54:41,834 - INFO - joeynmt.data - Data loaded.
2025-05-16 09:54:41,834 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=100000, src_lang=de, trg_lang=it, has_trg=True, random_subset=-1)
2025-05-16 09:54:41,834 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=923, src_lang=de, trg_lang=it, has_trg=True, random_subset=-1)
2025-05-16 09:54:41,834 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1567, src_lang=de, trg_lang=it, has_trg=True, random_subset=-1)
2025-05-16 09:54:41,834 - INFO - joeynmt.data - First training example:
	[SRC] Al Gor@@ e: Die Ab@@ wend@@ ung der Klima@@ katastrop@@ he
	[TRG] Al Gor@@ e: arrest@@ iamo il riscaldamento globale
2025-05-16 09:54:41,834 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) die (5) und (6) der (7) in (8) das (9) zu
2025-05-16 09:54:41,834 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) di (5) che (6) e (7) la (8) un (9) è
2025-05-16 09:54:41,834 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 20909
2025-05-16 09:54:41,834 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 19519
2025-05-16 09:54:41,839 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-16 09:54:41,987 - INFO - joeynmt.model - Enc-dec model built.
2025-05-16 09:54:41,989 - INFO - joeynmt.model - Total params: 13248768
2025-05-16 09:54:41,989 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=20909),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=19519),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2025-05-16 09:54:41,989 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2025-05-16 09:54:41,989 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2025-05-16 09:54:41,989 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2025-05-16 09:54:41,990 - INFO - joeynmt.training - EPOCH 1
2025-05-16 09:55:13,221 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     5.057142, Batch Acc: 0.051826, Tokens per Sec:     2044, Lr: 0.000300
2025-05-16 09:55:43,161 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     4.743062, Batch Acc: 0.061538, Tokens per Sec:     2075, Lr: 0.000300
2025-05-16 09:56:14,177 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     4.818749, Batch Acc: 0.066954, Tokens per Sec:     2055, Lr: 0.000300
2025-05-16 09:56:21,043 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-16 09:56:21,196 - INFO - joeynmt.model - Enc-dec model built.
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/homebrew/Cellar/python@3.10/3.10.17/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/Users/blueberry/Library/CloudStorage/OneDrive-UniversitätZürichUZH/Studium/FS25/MT/MT_exercises/MT_ex4/joeynmt/joeynmt/__main__.py", line 61, in <module>
    main()
  File "/Users/blueberry/Library/CloudStorage/OneDrive-UniversitätZürichUZH/Studium/FS25/MT/MT_exercises/MT_ex4/joeynmt/joeynmt/__main__.py", line 41, in main
    train(cfg_file=args.config_path, skip_test=args.skip_test)
  File "/Users/blueberry/Library/CloudStorage/OneDrive-UniversitätZürichUZH/Studium/FS25/MT/MT_exercises/MT_ex4/joeynmt/joeynmt/training.py", line 846, in train
    test(
  File "/Users/blueberry/Library/CloudStorage/OneDrive-UniversitätZürichUZH/Studium/FS25/MT/MT_exercises/MT_ex4/joeynmt/joeynmt/prediction.py", line 403, in test
    ckpt = resolve_ckpt_path(load_model, model_dir)
  File "/Users/blueberry/Library/CloudStorage/OneDrive-UniversitätZürichUZH/Studium/FS25/MT/MT_exercises/MT_ex4/joeynmt/joeynmt/helpers.py", line 508, in resolve_ckpt_path
    assert load_model.is_file(), load_model
AssertionError: /Users/blueberry/Library/CloudStorage/OneDrive-UniversitätZürichUZH/Studium/FS25/MT/MT_exercises/MT_ex4/mt-exercise-4/modelss/0.ckpt
